# 分布式锁 --sync.Mutex(单机锁) ,trylock, redis,etcd,zk,redlock  

### 架构
```text
01              进程内加锁
02trylock       trylock 
03setnx         基于 redis 的 setnx
```

### 1.进程内加锁(单机锁)

### 2.trylock 实现分布式锁
```text
在单机场景下，不建议使用这种锁。
在单机系统中，trylock 并不是一个好选择。因为大量的 goroutine 抢锁可能会导致
 cpu 无意义的资源浪费。有一个专有名词用来描述这种抢锁的场景：活锁。
活锁指的是程序看起来在正常执行，但实际上 cpu 周期被浪费在抢锁，而非执行任务上，
从而程序整体的执行效率低下。
```

### 3.基于 redis 的 setnx（ 依赖于这些请求到达 redis 节点的顺序来做正确的抢锁操作。）实现分布式锁
```text
通过代码和执行结果可以看到，我们远程调用 setnx 实际上和单机的 trylock 非常相似，
如果获取锁失败，那么相关的任务逻辑就不应该继续向前执行。

setnx 很适合在高并发场景下，用来争抢一些“唯一”的资源。
比如交易撮合系统中卖家发起订单，而多个买家会对其进行并发争抢。
这种场景我们没有办法依赖具体的时间来判断先后，因为不管是用户设备的时间，
还是分布式场景下的各台机器的时间，都是没有办法在合并后保证正确的时序的。
哪怕是我们同一个机房的集群，不同的机器的系统时间可能也会有细微的差别。

所以，我们需要 依赖于这些请求到达 redis 节点的顺序来做正确的抢锁操作。
如果用户的网络环境比较差，那也只能自求多福了。
```

### 4.基于 zk （zookeeper）实现分布式锁
```text
基于 zk 的锁与基于 redis 的锁的不同之处在于 Lock 成功之前会一直阻塞，这与我们单机场景中的 mutex.Lock 很相似。

其原理也是基于临时 sequence 节点和 watch api，例如我们这里使用的是 /lock 节点。Lock 会在该节点下的
节点列表中插入自己的值，只要节点下的子节点发生变化，就会通知所有 watch 该节点的程序。
这时候程序会检查当前节点下最小的子节点的 id 是否与自己的一致。如果一致，说明加锁成功了。

这种分布式的阻塞锁比较适合分布式任务调度场景，但不适合高频次持锁时间短的抢锁场景。
按照 Google 的 chubby 论文里的阐述，基于强一致协议的锁适用于 粗粒度 的加锁操作。
这里的粗粒度指锁占用时间较长。我们在使用时也应思考在自己的业务场景中使用是否合适。
```

### 5.基于 etcd 实现分布式锁
```text
etcd 中没有像 zookeeper 那样的 sequence 节点。所以其锁实现和基于 zookeeper 实现的有所不同。
etcdsync 的 Lock 流程是：
1.先检查 /lock 路径下是否有值，如果有值，说明锁已经被别人抢了
2.如果没有值，那么写入自己的值。写入成功返回，说明加锁成功。
  写入时如果节点被其它节点写入过了，那么会导致加锁失败，这时候到 3
3.watch /lock 下的事件，此时陷入阻塞
4.当 /lock 路径下发生事件时，当前进程被唤醒。
  检查发生的事件是否是删除事件(说明锁被持有者主动 unlock)，或者过期事件(说明锁过期失效)。
  如果是的话，那么回到 1，走抢锁流程。
```

### 6.redlock 实现分布式锁
```text
redlock 也是一种阻塞锁，单个节点操作对应的是 set nx px 命令，超过半数节点返回成功时，就认为加锁成功。
```

### 7.如何选择
```text
1.业务还在单机就可以搞定的量级时，那么按照需求使用任意的单机锁方案就可以。
2.如果发展到了分布式服务阶段，但业务规模不大，比如 qps < 1000，使用哪种锁方案都差不多。
  如果公司内已有可以使用的 zk/etcd/redis 集群，那么就尽量在不引入新的技术栈的情况下满足业务需求。
3.业务发展到一定量级的话，就需要从多方面来考虑了。首先是你的锁是否在任何恶劣的条件下都不允许数据丢失，
  如果不允许，那么就不要使用 redis 的 setnx 的简单锁。
4.如果要使用 redlock，那么要考虑你们公司 redis 的集群方案，
  是否可以直接把对应的 redis 的实例的 ip+port 暴露给开发人员。如果不可以，那也没法用。
5.对锁数据的可靠性要求极高的话，那只能使用 etcd 或者 zk 这种通过一致性协议保证数据可靠性的锁方案。
  但可靠的背面往往都是较低的吞吐量和较高的延迟。
  需要根据业务的量级对其进行压力测试，以确保分布式锁所使用的 etcd/zk 集群可以承受得住实际的业务请求压力。
  需要注意的是，etcd 和 zk 集群是没有办法通过增加节点来提高其性能的。
  要对其进行横向扩展，只能增加搭建多个集群来支持更多的请求。这会进一步提高对运维和监控的要求。
  多个集群可能需要引入 proxy，没有 proxy 那就需要业务去根据某个业务 id 来做 sharding。
  如果业务已经上线的情况下做扩展，还要考虑数据的动态迁移。这些都不是容易的事情。
```